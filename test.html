<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>GPT-2 Chatbot</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.0.0"></script>
</head>
<body>
  <h1>GPT-2 Chatbot</h1>
  <textarea id="input" rows="4" cols="50" placeholder="Ask something..."></textarea><br>
  <button id="send">Send</button>
  <h2>Response:</h2>
  <pre id="response"></pre>

  <script>
    // Load GPT-2 Model (Hosted from Hugging Face or other CDN)
    async function loadModel() {
      // URL for a GPT-2 model hosted on Hugging Face or any CDN with CORS enabled
      const modelURL = 'https://huggingface.co/gpt2/resolve/main/pytorch_model.bin';
      
      try {
        // You can replace the model URL with your own, or download and host the model locally
        const model = await tf.loadGraphModel(modelURL);  
        return model;
      } catch (error) {
        console.error("Failed to load model", error);
        return null;
      }
    }

    async function generateResponse(model, inputText) {
      if (!model) {
        return 'Model failed to load.';
      }
      
      // Here you can implement GPT-2 specific generation logic using the loaded model
      // This part is where you'd need to adapt the GPT-2 model output to the user's input
      // Example with a simple random response (to be replaced by actual inference logic)
      const response = `Response for: "${inputText}"\nThis is a placeholder response.`;
      return response;
    }

    document.getElementById('send').addEventListener('click', async () => {
      const userInput = document.getElementById('input').value;
      const responseElement = document.getElementById('response');
      responseElement.textContent = 'Thinking...';

      const model = await loadModel();
      const output = await generateResponse(model, userInput);
      responseElement.textContent = output;
    });
  </script>
</body>
</html>